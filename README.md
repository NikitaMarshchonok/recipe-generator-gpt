# ğŸ³ Recipe Generator GPT

This project is a **recipe generation model** built using a fine-tuned version of `distilgpt2` from Hugging Face Transformers. It takes a short input prompt and generates cooking recipes in natural language.

## ğŸ”§ Built With

- `distilgpt2` model from HuggingFace Transformers
- `pandas`, `datasets`, `transformers`, `gradio`
- Trained **locally on CPU** using MacBook Pro
- Used **~100,000 rows** extracted from the original RecipeNLG dataset (2.5GB) and preprocessed locally

## ğŸ“Š Dataset

- Based on the [RecipeNLG dataset](https://recipenlg.cs.put.poznan.pl/)
- For training, a **smaller subset (100,000 recipes)** was created from the full dataset for efficiency
- Data was preprocessed and cleaned to retain only the ingredients list

## ğŸ§  Training Details

- Fine-tuned locally using Hugging Face `Trainer`
- Used `TrainingArguments` with 3 epochs, batch size = 8, `distilgpt2` as the base
- Training completed on **MacBook Pro with M2 Pro chip (CPU only)**
- **Future training will be performed on GPU** to increase generation quality and performance

## ğŸ—º Roadmap

- âœ… Initial fine-tuning on CPU (done)
- ğŸ”„ Migrate training to **GPU (Colab / cloud)** for better results
- â• Add support for **image generation** via DALLÂ·E / Stable Diffusion
- ğŸŒ Deploy demo via **Gradio** or Hugging Face Spaces
- ğŸŒ Add multi-language recipe generation (e.g., ğŸ‡·ğŸ‡º Russian, ğŸ‡®ğŸ‡± Hebrew)

## ğŸš€ Demo (WIP)

Will be available soon via a [Gradio](https://www.gradio.app/) demo interface.

## ğŸ¤– Example

Input:

